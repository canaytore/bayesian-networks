[["index.html", "Decision Analysis - Bayesian Networks Chapter 1 Introduction 1.1 Aim 1.2 Motivation", " Decision Analysis - Bayesian Networks Author: Can Ayt√∂re Last Update: 2022-02-20 Chapter 1 Introduction 1.1 Aim 1.2 Motivation We have an understanding of BNs as graphical models representing probability distributions. What does that imply in terms of the underlying probability distribution? What happens if a probability distribution factorizes with respect to a graph? What kind of computations can we make on BNs? What kind of questions can we answer using (quantified) BNs? "],["getting-started-with.html", "Chapter 2 Getting Started with  2.1 What is BN? 2.2 Running Example: Is the Family Out?", " Chapter 2 Getting Started with  2.1 What is BN? A Probabilistic Network (aka causal graph, Bayesian belief network, etc.) is a graphical representation of a joint probability distribution. 2.2 Running Example: Is the Family Out? When Mr. West goes home at night, he wants to know if his family is home before trying the doors (maybe because the most convenient door to enter is double locked when nobody is home.) Often, when Mrs. West leaves the house, she turns on an outdoor light. However, she sometimes turns on this light if she is expecting a guest. Also (and of course!) the Wests have a dog. When nobody is home, the dog is put in the backyard. The same is true if the dog has bowel troubles. Finally, if the dog is in the backyard, Mr. West will probably hear her barking (or what he thinks is her barking), but sometimes he can be confused by other dogs barking. fo.data &lt;- readRDS(url(&quot;https://github.com/canaytore/bayesian-networks/raw/main/data/fo_data.rds&quot;)) #fo.data is imported head(fo.data) ## B D F H L ## 1 NO IN FALSE NO FALSE ## 2 NO OUT TRUE YES FALSE ## 3 NO IN FALSE NO FALSE ## 4 NO OUT TRUE NO FALSE ## 5 NO OUT FALSE YES FALSE ## 6 NO IN TRUE NO TRUE library(bnlearn) library(gRain) library(ggplot2) fo.dag &lt;- model2network(&quot;[F][B][L|F][D|F:B][H|D]&quot;) #Family-out network is created graphviz.plot(fo.dag) Each iteration results will be collected in following arrays. #First case: P(F=TRUE) first.mle &lt;- array(dim = 20) first.bayes &lt;- array(dim = 20) #Second case: P(D=OUT | B=YES, F=TRUE) second.mle &lt;- array(dim = 20) second.bayes &lt;- array(dim = 20) Now that we have a model ( fo.dag ) and data ( fo.data ) We can learn the conditional probability tables (parameters) using the bn.fit function which implements the maximum likelihood maximization and a Bayesian method to learn parameters. for(i in 1:20){ #P(F=TRUE) using mle: first.mle[i] &lt;- bn.fit(fo.dag, fo.data[1:(500*i),])$F$prob[&quot;TRUE&quot;] #P(F=TRUE) using bayes: first.bayes[i] &lt;- bn.fit(fo.dag, fo.data[1:(500*i),], method = &quot;bayes&quot;, iss=10)$F$prob[&quot;TRUE&quot;] #P(D=OUT | B=YES, F=TRUE) using mle: second.mle[i] &lt;- bn.fit(fo.dag, fo.data[1:(500*i),])$D$prob[&quot;OUT&quot;,&quot;YES&quot;,&quot;TRUE&quot;] #P(D=OUT | B=YES, F=TRUE) using bayes: second.bayes[i] &lt;- bn.fit(fo.dag, fo.data[1:(500*i),], method = &quot;bayes&quot;, iss=10)$D$prob[&quot;OUT&quot;,&quot;YES&quot;,&quot;TRUE&quot;] } Plotting P(F=TRUE) using mle: first.mle #Each iteration results ## [1] 0.1860000 0.1640000 0.1733333 0.1795000 0.1784000 0.1746667 0.1682857 ## [8] 0.1647500 0.1633333 0.1606000 0.1590909 0.1586667 0.1587692 0.1581429 ## [15] 0.1570667 0.1561250 0.1564706 0.1560000 0.1565263 0.1566000 first.mle &lt;- as.data.frame(first.mle) first.mle$ssize &lt;- 1:20 # add iteration column ggplot(first.mle, aes(ssize, first.mle)) + geom_line() + geom_hline(yintercept = 0.1566, color=&quot;red&quot;, size=1) + xlab(&quot;iteration&quot;) + ylab(&quot;P(F=TRUE) using MLE&quot;) + ylim(range(0.15,0.2)) Plotting P(F=TRUE) using bayes: first.bayes #Each iteration results ## [1] 0.1921569 0.1673267 0.1754967 0.1810945 0.1796813 0.1757475 0.1692308 ## [8] 0.1655860 0.1640798 0.1612774 0.1597096 0.1592346 0.1592934 0.1586305 ## [15] 0.1575233 0.1565543 0.1568743 0.1563818 0.1568875 0.1569431 first.bayes &lt;- as.data.frame(first.bayes) first.bayes$ssize &lt;- 1:20 # add iteration column ggplot(first.bayes, aes(ssize, first.bayes)) + geom_line() + geom_hline(yintercept = 0.1569431, color=&quot;red&quot;, size=1) + xlab(&quot;iteration&quot;) + ylab(&quot;P(F=TRUE) using BAYES&quot;) + ylim(range(0.15,0.2)) Plotting P(F=TRUE) comparing both mle and bayes methods: plot(first.mle[,1], type=&quot;l&quot;, col=&quot;red&quot;, lwd = 2, xlab=&quot;iteration&quot;, ylab=&quot;P(F=TRUE)&quot;, ylim=range(0.15,0.20), main=&quot;Plotting both methods&quot;) lines(first.bayes[,1], type=&quot;l&quot;, col=&quot;green&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;MLE&quot;, &quot;BAYES&quot;), col = c(&quot;red&quot;,&quot;green&quot;), bty=&#39;n&#39;, lty=1, lwd=2) Plotting P(D=OUT | B=YES, F=TRUE) using mle: second.mle #Each iteration results ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 second.mle &lt;- as.data.frame(second.mle) second.mle$ssize &lt;- 1:20 # add iteration column ggplot(second.mle, aes(ssize, second.mle)) + geom_line() + geom_hline(yintercept = 1, color=&quot;red&quot;, size=1) + xlab(&quot;iteration&quot;) + ylab(&quot;P(D=OUT | B=YES, F=TRUE) using MLE&quot;) + ylim(range(0,1)) Plotting P(D=OUT | B=YES, F=TRUE) using bayes: second.bayes #Each iteration results ## [1] 0.6428571 0.6428571 0.6428571 0.7222222 0.7727273 0.7727273 0.7727273 ## [8] 0.8076923 0.8333333 0.8529412 0.8529412 0.8809524 0.8809524 0.9000000 ## [15] 0.9000000 0.9074074 0.9193548 0.9242424 0.9324324 0.9324324 second.bayes &lt;- as.data.frame(second.bayes) second.bayes$ssize &lt;- 1:20 # add iteration column ggplot(second.bayes, aes(ssize, second.bayes)) + geom_line() + geom_hline(yintercept = 0.9324324, color=&quot;red&quot;, size=1) + xlab(&quot;iteration&quot;) + ylab(&quot;P(D=OUT | B=YES, F=TRUE) using BAYES&quot;) + ylim(range(0,1)) Plotting P(D=OUT | B=YES, F=TRUE) comparing both mle and bayes methods: plot(second.mle[,1], type=&quot;l&quot;, col=&quot;red&quot;, lwd = 2, xlab=&quot;iteration&quot;, ylab=&quot;P(D=OUT | B=YES, F=TRUE)&quot;, ylim=range(0.6,1), main=&quot;Plotting both methods&quot;) lines(second.bayes[,1], type=&quot;l&quot;, col=&quot;green&quot;, lwd = 2) legend(&quot;bottomright&quot;, legend = c(&quot;MLE&quot;, &quot;BAYES&quot;), col = c(&quot;red&quot;,&quot;green&quot;), bty=&#39;n&#39;, lty=1, lwd=2) "],["influence-diagrams.html", "Chapter 3 Influence Diagrams 3.1 Umbrella Problem 3.2 Incorporating Weather Forecast 3.3 Oil-Wildcatter Problem: 3.4 Extended Oil-Wildcatter Problem:", " Chapter 3 Influence Diagrams An influence diagram is: - a directed acyclic graph G containing decision and chance nodes, and information and relevance arcs representing what is known at the time of a decision and probabilistic dependency, respectively. - a set of probability distributions associated with each chance node, - optionally a utility node and a corresponding set of utilities 3.1 Umbrella Problem Consider the case when you are getting out of your apartment and you want to decide whether to take an umbrella with you or not. Your decision is certainly dependent on the weather, a phenomenon you cannot control. For simplicity, assume the weather can get one of the two states: sunshine or rain. You have likes and dislikes about taking your umbrella with you (you care!) depending on the weather. If it turns out to be a sunny day and you have taken your umbrella with you, you dont like it. However, you would hate it more if it rains and you do not have your umbrella. You are happier if it is sunny and you have left your umbrella at home (do not need to carry it around). You are happiest if it rains and you have your umbrella with you. library(HydeNet) umbrella.net &lt;- HydeNetwork(~ payoff | action*weather) For the chance node weather: weather_prob &lt;- c(0.3, 0.7) #Prior probability for rain and sunshine, respectively. umbrella.net &lt;- setNode(umbrella.net, weather, nodeType=&quot;dcat&quot;, pi=vectorProbs(p=weather_prob, weather), factorLevels = c(&quot;rain&quot;,&quot;sunshine&quot;), validate = FALSE) For the decision nodes action: dprob &lt;- c(1,0) #Probability of decision nodes represent do and don&#39;t. umbrella.net &lt;- setNode(umbrella.net, action, nodeType=&quot;dcat&quot;, pi=vectorProbs(p=dprob, action), factorLevels = c(&quot;take&quot;,&quot;leave&quot;), validate = FALSE) For the utility node payoff: umbrella.net &lt;- setNode(umbrella.net, payoff, &quot;determ&quot;, define=fromFormula(), nodeFormula = payoff ~ ifelse(weather == &quot;rain&quot;, ifelse(action == &quot;take&quot;, 100, 0), ifelse(action == &quot;take&quot;, 30, 70))) umbrella.net &lt;- setDecisionNodes(umbrella.net, action) #Setting the decision nodes umbrella.net &lt;- setUtilityNodes(umbrella.net, payoff) #Setting the utility nodes plot(umbrella.net) #Plotting the influence diagram of extended version of oil-wildcatter problem 3.1.1 Finding the optimal policy Let us pass this structure to the Gibbs sampler (JAGS) and track the variables of interest (these nodes are useful to compute the conditional expectations if necessary). Please note that the rjags package is just an interface to the JAGS library. Make sure you have installed JAGS-4.exe (can be downloaded from here) We are trying to compute the expected utility of the policy: (action). HydeNet provides a policyMatrix() function to enumerate all policies of a given decision network. This decision network is solved using Gibbs sampling with 10^6 samples. We could use them as follows: policy_umbrella &lt;- policyMatrix(umbrella.net) compiledNet_umbrella &lt;- compileDecisionModel(umbrella.net, policyMatrix = policy_umbrella) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 0 ## Unobserved stochastic nodes: 2 ## Total graph size: 16 ## ## Initializing model ## ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 1 ## Unobserved stochastic nodes: 1 ## Total graph size: 16 ## ## Initializing model ## ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 1 ## Unobserved stochastic nodes: 1 ## Total graph size: 16 ## ## Initializing model trackedVars_umbrella &lt;- c(&quot;weather&quot;,&quot;action&quot;,&quot;payoff&quot;) samples_umbrella &lt;- lapply(compiledNet_umbrella, HydeSim, variable.names = trackedVars_umbrella, n.iter = 10^5) Let us compute a single utility on samples: samples_umbrella &lt;- lapply(samples_umbrella, function(x) cbind(x, utility = x$payoff)) Now we can view all expectations at once by combining policies and the computed expectations: results_umbrella &lt;- lapply(samples_umbrella, function(l) mean(l$utility)) results_umbrella &lt;- as.data.frame(unlist(results_umbrella)) results_umbrella &lt;- cbind(policy_umbrella, results_umbrella) colnames(results_umbrella)[ncol(policy_umbrella)+1] &lt;- &quot;utility&quot; The results according policies are given below: results_umbrella[] ## action utility ## 1 1 50.8901 ## 2 2 48.9636 According to the policies, all utility values seem feasible and we dont have any conceptual error in the model. The optimal policy seems to be: results_umbrella[results_umbrella$utility == max(results_umbrella$utility),] ## action utility ## 1 1 50.8901 ..the first policy which means DO the action, or TAKE the UMBRELLA which results in an expected reward of app. 51.2%. As extra information to be obtained from the output (to double check the validity of this model in this case), Among the cases where it is decided not to ?drill? (3,4,7 and 8), it is sensible that the decision of not doing ?test? and not doing ?recovery? must be taken. It is clearly seen from result table. So when you do drill, you simply do not ?test?. The utility results can be seen from the following table: lapply(samples_umbrella, function(l) mean(l[l$weather==&quot;rain&quot;,]$utility)) ## [[1]] ## [1] 100 ## ## [[2]] ## [1] 0 lapply(samples_umbrella, function(l) mean(l[l$weather==&quot;sunshine&quot;,]$utility)) ## [[1]] ## [1] 30 ## ## [[2]] ## [1] 70 3.1.1.1 Evaluating Influence Diagrams: Augmented Variable Elimination Barren node removal: A barren node may be simply removed from an oriented, regular influence diagram. If it is a decision node, then any alternative would be optimal. Chance node removal: Given that chance node X directly precedes the value node and nothing else in an oriented, regular ID, node X can be removed by conditional expectation. Afterward, the value node inherits all of the conditional predecessors from node X, and thus the process creates no new barren nodes. Decision node removal: Given that all barren nodes have been removed, that decision node A is a conditional predecessor of the value node, and that all other conditional predecessors of the value node are informational predecessors of node A in an oriented, regular ID, node A may be removed by maximizing expected utility, conditioned on the values of its informational predecessors. The maximizing alternative(s) should be recorded as the optimal policy. Arc reversal: (between chance nodes). As we have seen before. 3.2 Incorporating Weather Forecast Consider the umbrella problem. Assume that you now decided to make a more informed decision. You listen to a weather forecast for that day. The new situation is depicted as: library(HydeNet) umbrella2.net &lt;- HydeNetwork(~ payoff | action*weather + action | weather_forecast + weather_forecast | weather) For the chance node weather: weather_prob &lt;- c(0.3,0.7) #Prior probability for rain and sunshine, respectively. umbrella2.net &lt;- setNode(umbrella2.net, weather, nodeType=&quot;dcat&quot;, pi=vectorProbs(p=weather_prob, weather), factorLevels = c(&quot;rain&quot;,&quot;sunshine&quot;), validate = FALSE) For the chance node weather_forecast: cpt_weather_forecast &lt;- readRDS(url(&quot;https://github.com/canaytore/bayesian-networks/raw/main/data/cpt_weather_forecast.rds&quot;)) #The file ?cpt_weather_forecast.rds? was created using the inputCPT() function of HydeNet. It stands for the conditional probability table of the seismic results based on parent nodes which are &#39;test&#39; and &#39;oil_content&#39;. cpt_weather_forecast ## weather_forecast ## weather sunny cloudy rainy ## rain 0.15 0.25 0.6 ## sunshine 0.70 0.20 0.1 umbrella2.net &lt;- setNodeModels(umbrella2.net, cpt_weather_forecast) For the decision nodes action: dprob &lt;- c(1,0) #Probability of decision nodes represent do and don&#39;t. umbrella2.net &lt;- setNode(umbrella2.net, action, nodeType=&quot;dcat&quot;, pi=vectorProbs(p=dprob, action), factorLevels = c(&quot;take&quot;,&quot;leave&quot;), validate = FALSE) For the utility node payoff: umbrella2.net &lt;- setNode(umbrella2.net, payoff, &quot;determ&quot;, define=fromFormula(), nodeFormula = payoff ~ ifelse(weather == &quot;rain&quot;, ifelse(action == &quot;take&quot;, 100, 0), ifelse(action == &quot;take&quot;, 30, 70))) umbrella2.net &lt;- setDecisionNodes(umbrella2.net, action) #Setting the decision nodes umbrella2.net &lt;- setUtilityNodes(umbrella2.net, payoff) #Setting the utility nodes plot(umbrella2.net) #Plotting the influence diagram of extended version of oil-wildcatter problem 3.2.1 Finding the optimal policy Let us pass this structure to the Gibbs sampler (JAGS) and track the variables of interest (these nodes are useful to compute the conditional expectations if necessary). Please note that the rjags package is just an interface to the JAGS library. Make sure you have installed JAGS-4.exe (can be downloaded from here) We are trying to compute the expected utility of the policy: (action). HydeNet provides a policyMatrix() function to enumerate all policies of a given decision network. This decision network is solved using Gibbs sampling with 10^6 samples. We could use them as follows: policy_umbrella2 &lt;- policyMatrix(umbrella2.net) compiledNet_umbrella2 &lt;- compileDecisionModel(umbrella2.net, policyMatrix = policy_umbrella2) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 0 ## Unobserved stochastic nodes: 3 ## Total graph size: 26 ## ## Initializing model ## ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 1 ## Unobserved stochastic nodes: 2 ## Total graph size: 26 ## ## Initializing model ## ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 1 ## Unobserved stochastic nodes: 2 ## Total graph size: 26 ## ## Initializing model trackedVars_umbrella2 &lt;- c(&quot;weather&quot;,&quot;weather_forecast&quot;,&quot;action&quot;,&quot;payoff&quot;) samples_umbrella2 &lt;- lapply(compiledNet_umbrella2, HydeSim, variable.names = trackedVars_umbrella2, n.iter = 10^5) Let us compute a single utility on samples: samples_umbrella2 &lt;- lapply(samples_umbrella2, function(x) cbind(x, utility = x$payoff)) Now we can view all expectations at once by combining policies and the computed expectations: results_umbrella2 &lt;- lapply(samples_umbrella2, function(l) mean(l$utility)) results_umbrella2 &lt;- as.data.frame(unlist(results_umbrella2)) results_umbrella2 &lt;- cbind(policy_umbrella2, results_umbrella2) colnames(results_umbrella2)[ncol(policy_umbrella2)+1] &lt;- &quot;utility&quot; The results according policies are given below: results_umbrella2[] ## action utility ## 1 1 51.0322 ## 2 2 48.8992 According to the policies, all utility values seem feasible and we dont have any conceptual error in the model. The optimal policy seems to be: results_umbrella2[results_umbrella2$utility == max(results_umbrella2$utility),] ## action utility ## 1 1 51.0322 ..the first policy which means DO the action, or TAKE the UMBRELLA which results in an expected reward of app. 51.2%. As extra information to be obtained from the output (also to double check the validity of this model in this case), Among the cases where it is decided not to ?drill? (3,4,7 and 8), it is sensible that the decision of not doing ?test? and not doing ?recovery? must be taken. It is clearly seen from result table. So when you do drill, you simply do not ?test?. The utility results can be seen from the following table: lapply(samples_umbrella2, function(l) mean(l[l$weather_forecast==&quot;sunny&quot;,]$utility)) ## [[1]] ## [1] 35.88221 ## ## [[2]] ## [1] 64.02782 lapply(samples_umbrella2, function(l) mean(l[l$weather_forecast==&quot;rainy&quot;,]$utility)) ## [[1]] ## [1] 80.68624 ## ## [[2]] ## [1] 19.45113 lapply(samples_umbrella2, function(l) mean(l[l$weather_forecast==&quot;cloudy&quot;,]$utility)) ## [[1]] ## [1] 54.61879 ## ## [[2]] ## [1] 45.52832 3.3 Oil-Wildcatter Problem: An oil wildcatter must decide whether to drill or not to drill. The cost of drilling is $70,000. If he decides to drill, the well may be soaking (with a return of $270,000), wet (with a return of $120,000), or dry (with a return of $0). These returns are exclusive of the drilling cost. The prior probabilities for soaking (lots of oil), wet (some oil), and dry (no oil) are (0.2, 0.3, 0.5). At the cost of $10,000, the oil wildcatter could decide to take seismic soundings of the geological structure at the site to classify the structure as having either no structure ns, open structure, os and closed structure cs. The specifics of the test are given as follows: cpt_test 3.3.1 Defining the extended network in HydeNet Our first task will be to represent and solve the extended oil-wildcatter influence diagram using HydeNet. We generate the network in HydeNet as follows; library(HydeNet) ow.net &lt;- HydeNetwork(~ cost | test + drill | test*seismic_results + seismic_results | test*oil_content + reward | drill*oil_content) For the chance node oil_content: oil_content_prob &lt;- c(0.5, 0.3, 0.2) #Prior probability for dry, wet and soaking, respectively. ow.net &lt;- setNode(ow.net, oil_content, nodeType=&quot;dcat&quot;, pi=vectorProbs(p=oil_content_prob, oil_content), factorLevels = c(&quot;dry&quot;,&quot;wet&quot;,&quot;soak&quot;), validate = FALSE) For the decision nodes test and drill: dprob &lt;- c(1,0) #Probability of decision nodes represent do and don&#39;t. ow.net &lt;- setNode(ow.net, test, nodeType=&quot;dcat&quot;, pi=vectorProbs(p=dprob, test), factorLevels = c(&quot;test&quot;,&quot;dont_test&quot;), validate = FALSE) ow.net &lt;- setNode(ow.net, drill, nodeType=&quot;dcat&quot;, pi=vectorProbs(p=dprob, drill), factorLevels = c(&quot;drill&quot;,&quot;dont_drill&quot;), validate = FALSE) For the chance node seismic_results: cpt_seismic_results &lt;- readRDS(url(&quot;https://github.com/canaytore/bayesian-networks/raw/main/data/cpt_seismic_results.rds&quot;)) #The file ?cpt_seismic_results.rds? was created using the inputCPT() function of HydeNet. It stands for the conditional probability table of the seismic results based on parent nodes which are &#39;test&#39; and &#39;oil_content&#39;. cpt_seismic_results ## , , seismic_results = ns ## ## oil_content ## test dry wet soak ## test 0.60 0.20 0.20 ## dont_test 0.33 0.33 0.33 ## ## , , seismic_results = os ## ## oil_content ## test dry wet soak ## test 0.30 0.40 0.30 ## dont_test 0.33 0.33 0.33 ## ## , , seismic_results = cs ## ## oil_content ## test dry wet soak ## test 0.10 0.40 0.50 ## dont_test 0.34 0.34 0.34 ow.net &lt;- setNodeModels(ow.net, cpt_seismic_results) For the utility node cost: ow.net &lt;- setNode(ow.net, cost, &quot;determ&quot;, define=fromFormula(), nodeFormula = cost ~ ifelse(test == &quot;test&quot;, -10, 0)) For the utility node reward: ow.net &lt;- setNode(ow.net, reward, &quot;determ&quot;, define=fromFormula(), nodeFormula = reward ~ ifelse(oil_content == &quot;dry&quot;, ifelse(drill == &quot;drill&quot;, -70, 0), ifelse(oil_content == &quot;wet&quot;, ifelse(drill == &quot;drill&quot;, 50, 0), ifelse(drill == &quot;drill&quot;, 200, 0)))) ow.net &lt;- setDecisionNodes(ow.net, test, drill) #Setting the decision nodes ow.net &lt;- setUtilityNodes(ow.net, cost, reward) #Setting the utility nodes plot(ow.net) #Plotting the influence diagram of extended version of oil-wildcatter problem The trick is here that we will have precise information about oil_content after the decision drill. Besides, there must be an arc from ?drill? to ?recovery_level? since recovery_level should exactly be nr (no recovery) when the drill is not done since it is not sensible to be able to do secondary recovery without the primary drill. 3.3.2 Finding the optimal policy Let us pass this structure to the Gibbs sampler (JAGS) and track the variables of interest (these nodes are useful to compute the conditional expectations if necessary). Please note that the rjags package is just an interface to the JAGS library. Make sure you have installed JAGS-4.exe (can be downloaded from here) We are trying to compute the expected utility of the policy: (action). HydeNet provides a policyMatrix() function to enumerate all policies of a given decision network. This decision network is solved using Gibbs sampling with 10^6 samples. We could use them as follows: policy_ow &lt;- policyMatrix(ow.net) compiledNet_ow &lt;- compileDecisionModel(ow.net, policyMatrix = policy_ow) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 0 ## Unobserved stochastic nodes: 4 ## Total graph size: 54 ## ## Initializing model ## ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 2 ## Unobserved stochastic nodes: 2 ## Total graph size: 50 ## ## Initializing model ## ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 2 ## Unobserved stochastic nodes: 2 ## Total graph size: 51 ## ## Initializing model ## ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 2 ## Unobserved stochastic nodes: 2 ## Total graph size: 51 ## ## Initializing model ## ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 2 ## Unobserved stochastic nodes: 2 ## Total graph size: 50 ## ## Initializing model trackedVars_ow &lt;- c(&quot;oil_content&quot;,&quot;seismic_results&quot;,&quot;test&quot;,&quot;cost&quot;,&quot;drill&quot;,&quot;reward&quot;) samples_ow &lt;- lapply(compiledNet_ow, HydeSim, variable.names = trackedVars_ow, n.iter = 10^5) Let us compute a single utility on samples: samples_ow &lt;- lapply(samples_ow, function(x) cbind(x, utility = x$reward + x$cost)) Now we can view all expectations at once by combining policies and the computed expectations: results_ow &lt;- lapply(samples_ow, function(l) mean(l$utility)) results_ow &lt;- as.data.frame(unlist(results_ow)) results_ow &lt;- cbind(policy_ow, results_ow) colnames(results_ow)[ncol(policy_ow)+1] &lt;- &quot;utility&quot; The results according policies are given below: results_ow[] ## test drill utility ## 1 1 1 9.6214 ## 2 2 1 20.2337 ## 3 1 2 -10.0000 ## 4 2 2 0.0000 According to the policies, all utility values seem feasible and we dont have any conceptual error in the model. The optimal policy seems to be: results_ow[results_ow$utility == max(results_ow$utility),] ## test drill utility ## 2 2 1 20.2337 ..the second policy which means DO NOT test, and DO drill which results in an expected reward of app. 19868.3$. As extra information to be obtained from the output (also to double check the validity of this model in this case), Among the cases where it is decided not to ?drill? (3,4,7 and 8), it is sensible that the decision of not doing ?test? and not doing ?recovery? must be taken. It is clearly seen from result table. So when you do drill, you simply do not ?test?. The utility results can be seen from the following table: lapply(samples_ow, function(l) mean(l[l$oil_content==&quot;dry&quot;,]$utility)) ## [[1]] ## [1] -80 ## ## [[2]] ## [1] -70 ## ## [[3]] ## [1] -10 ## ## [[4]] ## [1] 0 lapply(samples_ow, function(l) mean(l[l$oil_content==&quot;wet&quot;,]$utility)) ## [[1]] ## [1] 40 ## ## [[2]] ## [1] 50 ## ## [[3]] ## [1] -10 ## ## [[4]] ## [1] 0 lapply(samples_ow, function(l) mean(l[l$oil_content==&quot;soak&quot;,]$utility)) ## [[1]] ## [1] 190 ## ## [[2]] ## [1] 200 ## ## [[3]] ## [1] -10 ## ## [[4]] ## [1] 0 lapply(samples_ow, function(l) mean(l[l$seismic_results==&quot;ns&quot;,]$utility)) ## [[1]] ## [1] -35.42034 ## ## [[2]] ## [1] 20.0301 ## ## [[3]] ## [1] -10 ## ## [[4]] ## [1] 0 lapply(samples_ow, function(l) mean(l[l$seismic_results==&quot;os&quot;,]$utility)) ## [[1]] ## [1] 12.41259 ## ## [[2]] ## [1] 20.55022 ## ## [[3]] ## [1] -10 ## ## [[4]] ## [1] 0 lapply(samples_ow, function(l) mean(l[l$seismic_results==&quot;cs&quot;,]$utility)) ## [[1]] ## [1] 73.30904 ## ## [[2]] ## [1] 20.12121 ## ## [[3]] ## [1] -10 ## ## [[4]] ## [1] 0 3.4 Extended Oil-Wildcatter Problem: Extend the Oil Wildcatter problem to include the following: After drilling, striking oil, and extracting an optimal amount using primary recovery techniques, the wildcatter has the option of ex- tracting more oil using secondary recovery techniques at an additional cost of $20,000. Secondary recovery will result in no recovery (nr) with associated revenues of $0, low recovery (lr) with associated revenues of $30,000 or high recovery, (hr) with associated revenue of $50,000. The amount of secondary recovery depends on the amount of oil as well. If the well is wet, the conditional probabilities of nr, lr, and hr are 0.5, 0.4 and 0.1, respectively. If the well is soaking, the corresponding probabilities are 0.3, 0.5, 0.2. 3.4.1 Defining the extended network in HydeNet Our first task will be to represent and solve the extended oil-wildcatter influence diagram using HydeNet. We generate the network in HydeNet as follows; library(HydeNet) ow2.net &lt;- HydeNetwork(~ cost | test + drill | test*seismic_results + seismic_results | test*oil_content + reward | drill*oil_content + recovery | drill*seismic_results + extra_oil_content | oil_content + recovery_reward | recovery*extra_oil_content) For the chance node oil_content: oil_content_prob &lt;- c(0.5,0.3,0.2) #Prior probability for dry, wet and soaking. ow2.net &lt;- setNode(ow2.net, oil_content, nodeType=&quot;dcat&quot;, pi=vectorProbs(p=oil_content_prob, oil_content), factorLevels = c(&quot;dry&quot;,&quot;wet&quot;,&quot;soak&quot;), validate = FALSE) For the decision nodes test, drill and recovery: dprob &lt;- c(1,0) #Probability of decision nodes represent do and don&#39;t. ow2.net &lt;- setNode(ow2.net, test, nodeType=&quot;dcat&quot;, pi=vectorProbs(p=dprob, test), factorLevels = c(&quot;test&quot;,&quot;dont_test&quot;), validate = FALSE) ow2.net &lt;- setNode(ow2.net, drill, nodeType=&quot;dcat&quot;, pi=vectorProbs(p=dprob, drill), factorLevels = c(&quot;drill&quot;,&quot;dont_drill&quot;), validate = FALSE) ow2.net &lt;- setNode(ow2.net, recovery, nodeType=&quot;dcat&quot;, pi=vectorProbs(p=dprob, recovery), factorLevels = c(&quot;recovery&quot;,&quot;dont_recovery&quot;), validate = FALSE) For the chance node seismic_results: cpt_seismic_results &lt;- readRDS(url(&quot;https://github.com/canaytore/bayesian-networks/raw/main/data/cpt_seismic_results.rds&quot;)) #The file ?cpt_seismic_results.rds? was created using the inputCPT() function of HydeNet. It stands for the conditional probability table of the seismic results based on parent nodes which are &#39;test&#39; and &#39;oil_content&#39;. cpt_seismic_results ## , , seismic_results = ns ## ## oil_content ## test dry wet soak ## test 0.60 0.20 0.20 ## dont_test 0.33 0.33 0.33 ## ## , , seismic_results = os ## ## oil_content ## test dry wet soak ## test 0.30 0.40 0.30 ## dont_test 0.33 0.33 0.33 ## ## , , seismic_results = cs ## ## oil_content ## test dry wet soak ## test 0.10 0.40 0.50 ## dont_test 0.34 0.34 0.34 ow2.net &lt;- setNodeModels(ow2.net, cpt_seismic_results) For the chance node extra_oil_content: cpt_extra_oil_content &lt;- readRDS(url(&quot;https://github.com/canaytore/bayesian-networks/raw/main/data/cpt_extra_oil_content.rds&quot;)) #The file ?cpt_recovery_level.rds? was created using the inputCPT() function of HydeNet. It stands for the conditional probability table of the recovery levels based on parent nodes which are &#39;drill&#39; and &#39;oil_content&#39;. #The trick here is that &#39;recovery_level&#39; should exactly be nr (no recovery) when the drill is not done since it is not sensible to be able to do &#39;recovery&#39; without the &#39;drill&#39;. cpt_extra_oil_content ## extra_oil_content ## oil_content nr lr hr ## dry 0.2 0.1 0.7 ## wet 0.5 0.4 0.1 ## soak 0.3 0.5 0.2 ow2.net &lt;- setNodeModels(ow2.net, cpt_extra_oil_content) For the utility node cost: ow2.net &lt;- setNode(ow2.net, cost, &quot;determ&quot;, define=fromFormula(), nodeFormula = cost ~ ifelse(test == &quot;test&quot;, -10, 0)) For the utility node reward: ow2.net &lt;- setNode(ow2.net, reward, &quot;determ&quot;, define=fromFormula(), nodeFormula = reward ~ ifelse(oil_content == &quot;dry&quot;, ifelse(drill == &quot;drill&quot;, -70, 0), ifelse(oil_content == &quot;wet&quot;, ifelse(drill == &quot;drill&quot;, 50, 0), ifelse(drill == &quot;drill&quot;, 200, 0)))) For the utility node recovery_reward: ow2.net &lt;- setNode(ow2.net, recovery_reward, &quot;determ&quot;, define=fromFormula(), nodeFormula = recovery_reward ~ ifelse(extra_oil_content == &quot;nr&quot;, ifelse(recovery == &quot;recovery&quot;, -20, 0), ifelse(extra_oil_content == &quot;lr&quot;, ifelse(recovery == &quot;recovery&quot;, 10, 0), ifelse(recovery == &quot;recovery&quot;, 30, 0)))) ow2.net &lt;- setDecisionNodes(ow2.net, test, drill, recovery) #Setting the decision nodes ow2.net &lt;- setUtilityNodes(ow2.net, cost, reward, recovery_reward) #Setting the utility nodes plot(ow2.net) #Plotting the influence diagram of extended version of oil-wildcatter problem The trick is here that we will have precise information about oil_content after the decision drill. Besides, there must be an arc from ?drill? to ?recovery_level? since recovery_level should exactly be nr (no recovery) when the drill is not done since it is not sensible to be able to do secondary recovery without the primary drill. 3.4.2 Finding the optimal policy Let us pass this structure to the Gibbs sampler (JAGS) and track the variables of interest (these nodes are useful to compute the conditional expectations if necessary). Please note that the rjags package is just an interface to the JAGS library. Make sure you have installed JAGS-4.exe (can be downloaded from here) We are trying to compute the expected utility of the policy: (action). HydeNet provides a policyMatrix() function to enumerate all policies of a given decision network. This decision network is solved using Gibbs sampling with 10^6 samples. We could use them as follows: policy_ow2 &lt;- policyMatrix(ow2.net) compiledNet_ow2 &lt;- compileDecisionModel(ow2.net, policyMatrix = policy_ow2) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 0 ## Unobserved stochastic nodes: 6 ## Total graph size: 81 ## ## Initializing model ## ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 3 ## Unobserved stochastic nodes: 3 ## Total graph size: 76 ## ## Initializing model ## ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 3 ## Unobserved stochastic nodes: 3 ## Total graph size: 77 ## ## Initializing model ## ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 3 ## Unobserved stochastic nodes: 3 ## Total graph size: 77 ## ## Initializing model ## ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 3 ## Unobserved stochastic nodes: 3 ## Total graph size: 77 ## ## Initializing model ## ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 3 ## Unobserved stochastic nodes: 3 ## Total graph size: 77 ## ## Initializing model ## ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 3 ## Unobserved stochastic nodes: 3 ## Total graph size: 77 ## ## Initializing model ## ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 3 ## Unobserved stochastic nodes: 3 ## Total graph size: 77 ## ## Initializing model ## ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 3 ## Unobserved stochastic nodes: 3 ## Total graph size: 76 ## ## Initializing model trackedVars_ow2 &lt;- c(&quot;oil_content&quot;,&quot;seismic_results&quot;,&quot;test&quot;,&quot;cost&quot;,&quot;drill&quot;,&quot;recovery&quot;,&quot;reward&quot;,&quot;extra_oil_content&quot;,&quot;recovery_reward&quot;) samples_ow2 &lt;- lapply(compiledNet_ow2, HydeSim, variable.names = trackedVars_ow2, n.iter = 10^5) Let us compute a single utility on samples: samples_ow2 &lt;- lapply(samples_ow2, function(x) cbind(x, utility = x$reward + x$cost + x$recovery_reward)) Now we can view all expectations at once by combining policies and the computed expectations: results_ow2 &lt;- lapply(samples_ow2, function(l) mean(l$utility)) results_ow2 &lt;- as.data.frame(unlist(results_ow2)) results_ow2 &lt;- cbind(policy_ow2, results_ow2) colnames(results_ow2)[ncol(policy_ow2)+1] &lt;- &quot;utility&quot; The results according policies are given below: results_ow2[] ## test drill recovery utility ## 1 1 1 1 19.5100 ## 2 2 1 1 29.2822 ## 3 1 2 1 -0.8786 ## 4 2 2 1 9.0800 ## 5 1 1 2 10.2388 ## 6 2 1 2 19.8986 ## 7 1 2 2 -10.0000 ## 8 2 2 2 0.0000 According to the policies, all utility values seem feasible and we dont have any conceptual error in the model. The optimal policy seems to be: results_ow2[results_ow2$utility == max(results_ow2$utility),] ## test drill recovery utility ## 2 2 1 1 29.2822 ..the second policy which means DO NOT test, DO drill, and DO recovery which results in an expected reward of app. 29254.8$. As extra information to be obtained from the output (also to double check the validity of this model in this case), Among the cases where it is decided not to ?drill? (3,4,7 and 8), it is sensible that the decision of not doing ?test? and not doing ?recovery? must be taken. It is clearly seen from result table. So when you do drill, you simply do not ?test?. The utility results can be seen from the following table: lapply(samples_ow2, function(l) mean(l[l$oil_content==&quot;dry&quot;,]$utility)) ## [[1]] ## [1] -62.03975 ## ## [[2]] ## [1] -52.05873 ## ## [[3]] ## [1] 8.0135 ## ## [[4]] ## [1] 18.12075 ## ## [[5]] ## [1] -80 ## ## [[6]] ## [1] -70 ## ## [[7]] ## [1] -10 ## ## [[8]] ## [1] 0 lapply(samples_ow2, function(l) mean(l[l$oil_content==&quot;wet&quot;,]$utility)) ## [[1]] ## [1] 37.01576 ## ## [[2]] ## [1] 46.82637 ## ## [[3]] ## [1] -13.0174 ## ## [[4]] ## [1] -3.049024 ## ## [[5]] ## [1] 40 ## ## [[6]] ## [1] 50 ## ## [[7]] ## [1] -10 ## ## [[8]] ## [1] 0 lapply(samples_ow2, function(l) mean(l[l$oil_content==&quot;soak&quot;,]$utility)) ## [[1]] ## [1] 194.9721 ## ## [[2]] ## [1] 205.1909 ## ## [[3]] ## [1] -5.091019 ## ## [[4]] ## [1] 4.919315 ## ## [[5]] ## [1] 190 ## ## [[6]] ## [1] 200 ## ## [[7]] ## [1] -10 ## ## [[8]] ## [1] 0 lapply(samples_ow2, function(l) mean(l[l$seismic_results==&quot;ns&quot;,]$utility)) ## [[1]] ## [1] -21.9302 ## ## [[2]] ## [1] 29.53396 ## ## [[3]] ## [1] 3.55675 ## ## [[4]] ## [1] 8.945417 ## ## [[5]] ## [1] -35.04353 ## ## [[6]] ## [1] 19.47434 ## ## [[7]] ## [1] -10 ## ## [[8]] ## [1] 0 lapply(samples_ow2, function(l) mean(l[l$seismic_results==&quot;os&quot;,]$utility)) ## [[1]] ## [1] 21.30971 ## ## [[2]] ## [1] 28.98038 ## ## [[3]] ## [1] -2.029637 ## ## [[4]] ## [1] 9.20438 ## ## [[5]] ## [1] 13.12386 ## ## [[6]] ## [1] 20.30598 ## ## [[7]] ## [1] -10 ## ## [[8]] ## [1] 0 lapply(samples_ow2, function(l) mean(l[l$seismic_results==&quot;cs&quot;,]$utility)) ## [[1]] ## [1] 78.18684 ## ## [[2]] ## [1] 29.33302 ## ## [[3]] ## [1] -6.132648 ## ## [[4]] ## [1] 9.089307 ## ## [[5]] ## [1] 73.76556 ## ## [[6]] ## [1] 19.91059 ## ## [[7]] ## [1] -10 ## ## [[8]] ## [1] 0 lapply(samples_ow2, function(l) mean(l[l$extra_oil_content==&quot;nr&quot;,]$utility)) ## [[1]] ## [1] 10.7497 ## ## [[2]] ## [1] 20.09325 ## ## [[3]] ## [1] -30 ## ## [[4]] ## [1] -20 ## ## [[5]] ## [1] 30.92412 ## ## [[6]] ## [1] 39.52602 ## ## [[7]] ## [1] -10 ## ## [[8]] ## [1] 0 lapply(samples_ow2, function(l) mean(l[l$extra_oil_content==&quot;lr&quot;,]$utility)) ## [[1]] ## [1] 83.25936 ## ## [[2]] ## [1] 93.20159 ## ## [[3]] ## [1] 0 ## ## [[4]] ## [1] 10 ## ## [[5]] ## [1] 73.16703 ## ## [[6]] ## [1] 82.96354 ## ## [[7]] ## [1] -10 ## ## [[8]] ## [1] 0 lapply(samples_ow2, function(l) mean(l[l$extra_oil_content==&quot;hr&quot;,]$utility)) ## [[1]] ## [1] -15.49039 ## ## [[2]] ## [1] -4.964774 ## ## [[3]] ## [1] 20 ## ## [[4]] ## [1] 30 ## ## [[5]] ## [1] -45.64758 ## ## [[6]] ## [1] -35.69171 ## ## [[7]] ## [1] -10 ## ## [[8]] ## [1] 0 "],["references.html", "References", " References "],["appendices.html", "Appendices About Author", " Appendices About Author "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
